<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="/css/reset.css" type="text/css">
    <link rel="stylesheet" href="/css/vars.css" type="text/css">
    <link rel="stylesheet" href="/css/base.css" type="text/css">

    <!-- sidebar -->
    <link rel="stylesheet" href="/css/nav.css" type="text/css">

    <!-- syntax highlighting -->
    <link href="https://unpkg.com/prismjs@1.20.0/themes/prism-okaidia.css" rel="stylesheet">

    <!-- font -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;1,100;1,300&display=swap" rel="stylesheet"> 

    <!-- Dark mode : script 1 of 2-->
    <script src="/css/dark_mode_init.js"></script>

    <title>Doing distributed data analysis on a Raspberry Pi cluster</title>
  </head>
  <body>

    <div class = "container">
      <div class = "header">
      </div>

      <div class = "body">
      <div class = "sidebar">
      <link rel="stylesheet" href="/css/nav.css" type="text/css">

<!-- Switch for dark mode toggle -->
<link rel="stylesheet" href="/css/switch.css" type="text/css">

<div>
 <a class = "page-title" href = "/"> åŠ‰ </a>
 <span class = "page-subtitle"> Lieu Zheng Hong </span>
</div>


<hr>


<ul class = "navlist">
    <li> <a href="/"> Home </a> </li>
    <li> <a href="/about"> About </a> </li>
    <li> <a href="/projects"> Projects </a> </li>
    <li> <a href="/archive"> Archive </a> </li>
    <li> <a href="/ppe-stuff"> PPE work </li>
    <li> <a href="/notes"> Notes </a> </li>
</ul>

<hr>


<div class = "dark-mode-toggle-area">

  dark mode:
  <label id="theme-switch" class="theme-switch" for="checkbox_theme">
  <input type="checkbox" id="checkbox_theme" class="switch">
  <span class="slider"></span>
  </label>

</div>

      </div>

      <div class = "content-wrapper">
        <div class = "content">
          <h1> Doing distributed data analysis on a Raspberry Pi cluster </h1>
          <p>(Summer 2019 internship with Inzura)</p>
<hr>
<p>This is the second project I've done with Raspberry Pis. The first one was a
<a href="/2017/05/30/building-a-raspberry-pi-console.html">Raspberry Pi game console</a>
done back in 2017.</p>
<p>As far as I know, this is the largest Apache Spark Raspi cluster that has been
published online. There's an <a href="https://dev.to/awwsmm/building-a-raspberry-pi-hadoop-spark-cluster-8b2">eight-node Raspi 4 cluster
here</a>
and a <a href="https://www.raspberrypi.org/magpi/pi-spark-supercomputer/">six-node Raspi 2 cluster
here</a> but i) ours is
the biggest and ii) we have actually used the cluster to do &quot;real&quot; data
analysis.</p>
<h2>Results</h2>
<p>I calculated average traffic speeds of a sample of 50,000 trips in the UK.
These trips were recorded from drivers who used Inzura's mobile app. The
vast majority of trips were in Northern Ireland, and I've chosen to zoom in on
the traffic data in Belfast, Northern Ireland. These are <a href="https://en.wikipedia.org/wiki/Choropleth_map">chloropleth
maps</a>: roads with low average
speeds are dark purple, and roads with high average speeds bright yellow.</p>
<p><img src="/img/raspi-cluster/UK_cropped.png" alt=""></p>
<p>This is a really high-resolution image, best opened and viewed in full-size.
It shows the extent of coverage of the trips that I sampled. We can see that
while the data covers the entirety of the UK, most of the trips are from
Northern Ireland (NI).</p>
<p>As we zoom into NI we see that almost the entire landmass has been traversed.
We can clearly make out city centers, and the roads that connect them: Belfast
is by far the largest and most prominent---although one can make out other
cities like Bangor and Lisburn near it. We can also see Donegal and Omagh
in the west.</p>
<p><img src="/img/raspi-cluster/ireland_big.png" alt=""></p>
<p>We can glean some insights even at this distance. We can see that
average speeds in cities tend to be low (dark purple), while roads that connect
cities have higher average speeds.</p>
<p>Let's zoom in a little:</p>
<p><img src="/img/raspi-cluster/belfast_1000000.jpg" alt=""></p>
<p>We can now clearly see the shape of Belfast and the cities near it. There's an
interesting funnel shape at the top, almost like it's feeding into Belfast
proper. There's also what looks like extensions of the city to the south-west:
are these part of Belfast too, or cities in their own right?</p>
<p>From the road network alone we can see that towns usually set up near the
coast: the road network on both sides of the bay is much denser than bits
closer inland.</p>
<p>Now let's zoom in to Belfast proper:</p>
<p><img src="/img/raspi-cluster/belfast_250000.jpg" alt=""></p>
<p>At this zoom level the layout of the city can clearly be discerned. Streets
near the center of the city are laid out in a neat and densely-packed grid
pattern, which gives way to a sparser, more organic layout toward the</p>
<p>We can even see some ferry routes from the pier! These are routes to the
mainland and the Isle of Man.</p>
<p>Most interestingly, we can see an S-shaped road that snakes through the city.
It starts from the southwest and winds its way up northeast. We can see that it
is a different colour from the rest of the city's roads, which are mostly dark
purple: this road has segments of yellow and green.</p>
<p>Here's the same image with the background darkened so we can see the colours
more clearly:</p>
<p><img src="/img/raspi-cluster/belfast_250000_dark.jpg" alt=""></p>
<p>We can really make out the S-shaped road here, and a couple more besides.
There's a road leading down from the northeast that looks like a main
connection to the northeastern coast.</p>
<p>One final zoom in to 1:25000 scale:</p>
<p><img src="/img/raspi-cluster/belfast_100000.jpg" alt=""></p>
<p>The shape (and colour) of the S-shaped road is apparent now.</p>
<p><img src="/img/raspi-cluster/belfast_25000_dark.jpg" alt=""></p>
<p>Average speeds on the S-shaped road decrease when there is a bend in the road,
and when there is an intersection. Notice also that roads on the periphery tend
to have higher average velocities, although this could be because there is not
enough data on them (thus throwing off the averages).</p>
<p>One final render without the background---just because it looks pretty, and
shows us the shape of the city:</p>
<p><img src="/img/raspi-cluster/belfast_250000_white.jpg" alt=""></p>
<h2>Raspberry Pi compute cluster</h2>
<p>This is the Raspberry Pi compute cluster. It's taller than a man:</p>
<p><img src="/img/raspi-cluster/full-shot.jpg" alt="full body shot of cluster"></p>
<p>It was designed and built by Richard Jelbert, the CEO and co-founder of
Inzura. Almost everything you see is 3D-printed: that includes the
faceplates, the clips and the holding rack (for the Raspis). It contains 16
Raspi 4s and 25 assorted Raspi 3B/3/2/1s. For this project I only used the
Raspi 4s.</p>
<p>The structure of the cluster is hexagonal: there are 5 storeys and 6 segments
on each storey. One of the segments is reserved for SSDs (data storage and
database lookups), but the other 5 segments can be used for compute.</p>
<p><img src="/img/raspi-cluster/cluster_closeup.jpg" alt="close up shot of cluster"></p>
<p><em>I'm sorry Dave, I'm afraid I can't do that</em></p>
<p>The front panel opens up to expose the internals:</p>
<p><img src="/img/raspi-cluster/cluster_opened.jpg" alt="shot of cluster innards"></p>
<p>Again, all of this is 3D-printed. 8 Raspberry Pi 4s sit on each rack and are
cooled by 4 mini-fans per rack (the cooling is necessary to prevent them
throttling).</p>
<p><img src="/img/raspi-cluster/cluster_opened_closeup.jpg" alt="close up shot of cluster innards"></p>
<p>The cluster has a theoretical maximum capacity of 400 Raspis (16 Raspis per
segment, 5 segments per storey, 5 storeys total); Richard said that cooling
may be tricky at that density, but 12 Raspis per segment is definitely
doable. That gives us an actual capacity of 300 Raspis in the cluster.</p>
<p>It is also <a href="https://docs.min.io/docs/deploy-minio-on-docker-swarm.html">possible to cluster MinIO across up to 16
nodes</a>. Minio
allows distributed object storage: it lets my code run without having a copy of
every single file on every single Raspi. Given that we need only 1 SSD per
storey, this will be sufficient for our purposes.</p>
<h2>Why use the cluster?</h2>
<p>Why go to all the trouble to set up a compute cluster? Why not run it on a
single machine?</p>
<ol>
<li>It's the only scalable solution when the number of trips gets large</li>
<li>We get increased performance by parallelising computation</li>
<li>It's cool!</li>
</ol>
<h3>Scalability</h3>
<p>I had only been able to download a small sample of 50,000 anonymised trips.
But Inzura has a corpus of millions of trips and growing. It certainly
doesn't fit on a single machine's memory (it doesn't even fit on disk). So
the most naive approach of reading all the files into memory at once and
calculating the average that way won't work. Something that <em>does</em> work is to
read the files in one by one and calculate the averages that way, but this is
of course rather slow. Which leads us to ...</p>
<h3>Performance</h3>
<p>The task is to calculate average road speed for every road in the database.
This is an <em>embarassingly parallelisable</em> task: each trip is an individual JSON
file (which means we don't have to worry about shared memory), and there is
almost no work needed to combine the results of disparate parallel
computations. This means that we can get a close-to-linear speedup simply by
adding more computers --- which means that a cluster of Raspberry Pis will
outperform even a beefy (and much more expensive) computer.</p>
<p>Even if 16 Raspis don't outperform a single computer, 300 Raspis certainly
will, and it is easy now to add more Raspis now that the groundwork has been
laid. This opens the door to all sorts of future data analysis and exploration
tasks which may be prohibitively expensive for a single computer to run (in
terms of wall-clock time).</p>
<h3>Cool factor</h3>
<p>Why not just use Google Cloud or something?---because this is cooler and more
educational.</p>
<h2>Configuring the cluster</h2>
<p>Configuring the cluster was by far the hardest part of this project. Any
instructions I could find for setting up a Spark cluster were either
incomplete, out of date, or incorrect. It took Richard, Chris and I three full
days to set everything up. Eventually I heavily modified <a href="https://github.com/minio/cookbook/blob/master/docs/apache-spark-with-minio.md">this MinIO
cookbook</a>
and successfully deployed Spark on the 16 worker Raspis:</p>
<p><img src="/img/raspi-cluster/spark-console.jpg" alt="spark console with 16 slaves"></p>
<p>The cluster computer has 64 cores and 64GB of memory (45GB free)---already much better than
my laptop.</p>
<p>I have written a bash script that should automate (but not completely) the
installation process
<a href="https://gist.github.com/lieuzhenghong/c062aa2c5544d6b1a0fa5139e10441ad">here</a>.
The only thing one has to do apart from this is to edit the <code>core-site.xml</code>
file to point to one's S3 bucket/MinIO server.</p>
<h2>Apache Spark code</h2>
<p>I gained the necessary knowledge to do this data analysis project by working
through part of the <a href="https://www.coursera.org/specializations/scala">Functional Programming in Scala
Specialisation</a>. I finished the
courses <em>Functional Programming Principles in Scala</em>, <em>Parallel Programming</em>
and <em>Big Data Analysis with Scala and Spark</em>. The last course was the most
directly relevant to this project but I found the first course incredibly
helpful for learning Scala (and the functional programming paradigm), and the
third course for reasoning about parallel programs. Both courses helped me
appreciate why Scala is a good language for data analysis---many of the
functional abstractions of mapping over an iterable of some sort carry over
almost directly to distributed computing.</p>
<p>I found Spark quite concise, although the documentation was quite lacking.
Nonetheless, I was able to write the program in only about 20 lines of code.</p>
<p>The code does the following:</p>
<ol>
<li>Read a JSON file onto the Raspi</li>
<li>Extract all the roads and velocities from the JSON file (Map step)</li>
<li>Combine all the roads and find the average velocities from all the JSON
files (Reduce step)</li>
<li>Saves it into a single CSV file (the <code>coalesce</code> step). This step is optional
(and in fact is probably not recommended when the data set gets larger). I
did it to make it easier to visualise in QGIS.</li>
</ol>
<p>In total there were 50,000 trips, ~85 million data points, and ~170,000 unique
road IDs.</p>
<pre class="language-scala"><code class="language-scala"><span class="token keyword">object</span> SimpleApp <span class="token punctuation">{</span><br>  <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql</span><span class="token punctuation">.</span>SparkSession<br>    <span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>explode</span><br>    <span class="token keyword">val</span> spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">"TripAnalysis"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span><br>    <span class="token keyword">import</span> <span class="token namespace">spark<span class="token punctuation">.</span>implicits</span><span class="token punctuation">.</span>_<br>    <span class="token keyword">val</span> results_path <span class="token operator">=</span> <span class="token string">"s3a://results/"</span><br>    <span class="token keyword">val</span> paths <span class="token operator">=</span> <span class="token string">"s3a://trips/*"</span><br>    <span class="token keyword">val</span> tripDF <span class="token operator">=</span> spark<span class="token punctuation">.</span>read<span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"multiline"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>json<span class="token punctuation">(</span>paths<span class="token punctuation">)</span><br>	<span class="token comment">// "Explode" the data array into individual rows</span><br>    <span class="token keyword">val</span> linksDF <span class="token operator">=</span> tripDF<span class="token punctuation">.</span>select<span class="token punctuation">(</span>explode<span class="token punctuation">(</span>$<span class="token string">"data"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token keyword">val</span> linksDF2 <span class="token operator">=</span> linksDF<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">"data.dbResponse.linkID"</span><span class="token punctuation">,</span> <span class="token string">"data.absVelocity"</span><span class="token punctuation">)</span><br>    <span class="token operator">/</span><span class="token operator">/</span> create a temporary view using the DataFrame<br>    linksDF2<span class="token punctuation">.</span>createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"times"</span><span class="token punctuation">)</span><br>    <span class="token comment">/*<br>      root<br>      |-- linkID: string (nullable = true)<br>      |-- absVelocity: double (nullable = true)<br>    */</span><br>    <span class="token keyword">val</span> tDF <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>"SELECT CAST<span class="token punctuation">(</span>linkID as LONG<span class="token punctuation">)</span><span class="token punctuation">,</span> absVelocity from times<br>	WHERE linkID IS NOT NULL AND absVelocity IS NOT NULL"<span class="token punctuation">)</span><br>    <span class="token keyword">val</span> groupedDS <span class="token operator">=</span> tDF<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token string">"linkID"</span><span class="token punctuation">)</span><br>    <span class="token keyword">val</span> avgsDS <span class="token operator">=</span> groupedDS<span class="token punctuation">.</span>agg<span class="token punctuation">(</span><br>      <span class="token string">"linkID"</span> <span class="token operator">-></span> <span class="token string">"count"</span><span class="token punctuation">,</span><br>      <span class="token string">"absVelocity"</span> <span class="token operator">-></span> <span class="token string">"avg"</span><br>    <span class="token punctuation">)</span><span class="token punctuation">.</span>sort<span class="token punctuation">(</span>$<span class="token string">"linkID"</span><span class="token punctuation">.</span>asc<span class="token punctuation">)</span><br><br>    avgsDS<span class="token punctuation">.</span>coalesce<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">.</span><br>    option<span class="token punctuation">(</span><span class="token string">"header"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><br>    csv<span class="token punctuation">(</span>results_path <span class="token operator">+</span> <span class="token string">"results_49998"</span><span class="token punctuation">)</span><br>    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre>
<p>The Spark code gives the road IDs, but we still have to get the coordinates
that that road ID corresponds to. So I queried the road database (which also
runs in the cluster) with a couple of lines of throwaway Python code.</p>
<p>I had written a SQL utility function for my <a href="/2019/09/14/using-thompson-sampling-to-optimise-SMS-effectiveness.html">previous project on optimising SMS
messages</a>
which I was able to reuse here.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> time<br><span class="token keyword">from</span> sql_utils <span class="token keyword">import</span> query<br><br><span class="token comment"># First read csv</span><br><br><span class="token keyword">import</span> csv<br><br>rows <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"results-49998.csv"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    csvreader <span class="token operator">=</span> csv<span class="token punctuation">.</span>reader<span class="token punctuation">(</span>f<span class="token punctuation">)</span><br>    header <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span>csvreader<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><br>    <span class="token keyword">print</span><span class="token punctuation">(</span>header<span class="token punctuation">)</span><br>    <span class="token keyword">for</span> row <span class="token keyword">in</span> csvreader<span class="token punctuation">:</span><br>        rows<span class="token punctuation">.</span>append<span class="token punctuation">(</span>row<span class="token punctuation">)</span><br><br>BATCH_SIZE <span class="token operator">=</span> <span class="token number">10000</span><br>NUM_ROWS <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>rows<span class="token punctuation">)</span><br><br>pointer <span class="token operator">=</span> <span class="token number">0</span><br><br><span class="token comment"># Break up the rows</span><br><br>link_ids <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> rows<span class="token punctuation">]</span><br><br>all_start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><br><span class="token keyword">while</span> pointer <span class="token operator">&lt;</span> NUM_ROWS<span class="token punctuation">:</span><br>    start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><br>    link_ids_subset <span class="token operator">=</span> link_ids<span class="token punctuation">[</span>pointer<span class="token punctuation">:</span>pointer<span class="token operator">+</span>BATCH_SIZE<span class="token punctuation">]</span><br>    array_string <span class="token operator">=</span> <span class="token string">", "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> link_ids_subset<span class="token punctuation">)</span><br>    array_string <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"(</span><span class="token interpolation"><span class="token punctuation">{</span>array_string<span class="token punctuation">}</span></span><span class="token string">)"</span></span><br>    query_string <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"SELECT geom from (SELECT DISTINCT(link_id), "</span></span> <span class="token operator">+</span><br>	<span class="token string">"geom FROM streets WHERE link_id IN {array_string}) as f"</span><br><br>    <span class="token comment"># Send out the query</span><br>    query_result <span class="token operator">=</span> <span class="token punctuation">(</span>query<span class="token punctuation">(</span>query_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>pointer<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token punctuation">(</span>pointer<span class="token operator">+</span>BATCH_SIZE<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>rows<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>        rows<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>query_result<span class="token punctuation">[</span>i<span class="token operator">-</span>pointer<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br><br>    end <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><br>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Time taken to do </span><span class="token interpolation"><span class="token punctuation">{</span>pointer<span class="token punctuation">}</span></span><span class="token string">:</span><span class="token interpolation"><span class="token punctuation">{</span>pointer<span class="token operator">+</span>BATCH_SIZE<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>end<span class="token operator">-</span>start<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><br><br>    pointer <span class="token operator">+=</span> BATCH_SIZE<br><br>all_end <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Time taken to make all the queries: </span><span class="token interpolation"><span class="token punctuation">{</span>all_end<span class="token operator">-</span>all_start<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><br><br>header<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"geom"</span><span class="token punctuation">)</span><br><br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"results-49998-with-geom.csv"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> header<span class="token punctuation">)</span><span class="token punctuation">)</span><br>    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span><br><br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"results-49998-with-geom.csv"</span><span class="token punctuation">,</span> <span class="token string">"a"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    <span class="token keyword">for</span> row <span class="token keyword">in</span> rows<span class="token punctuation">[</span><span class="token punctuation">:</span>NUM_ROWS<span class="token punctuation">]</span><span class="token punctuation">:</span><br>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> row<span class="token punctuation">)</span><span class="token punctuation">)</span><br>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span></code></pre>
<p>There are issues with such an approach: when the number of unique road IDs get
large, we won't be able to load all the rows into memory like I do here. But
there were only ~140,000 unique link IDs, and I think even if you were to cover
the whole of the UK the number of links would not exceed 2 million --- which
fits well within the memory of a single machine.</p>
<p>I made two main optimisations.</p>
<p>First of all, I batched up the queries (instead of querying <code>SELECT * FROM streets WHERE link_ID = linkid</code>, do <code>WHERE link_ID IN {array_string}</code>) because
I knew that latency would be the primary bottleneck. This gave a huge speedup:
making 10,000 queries (without caching) only took about 1.2 seconds. To give a
comparison, measured round-trip time was about 0.1s (100ms); if I had made
140,000 individual queries, this would have taken 14,000 seconds (4 hours!),
and I was able to do all the queries in 14 seconds.</p>
<p>Second, I knew that the linkIDs were indexed in the database, and so I made
sure to sort the linkIDs before batching and sending them to maximise the
probability of a cache hit.</p>
<pre><code>(base) lieu@bigbeast:~/dev/cluster-computing$ python3 get_road_geom.py
['linkID', 'count(linkID)', 'avg(absVelocity)']
Time taken to do 0:10000: 0.5610289573669434
Time taken to do 10000:20000: 0.48032593727111816
Time taken to do 20000:30000: 0.5004429817199707
Time taken to do 30000:40000: 0.5266928672790527
Time taken to do 40000:50000: 0.5340712070465088
Time taken to do 50000:60000: 0.6023671627044678
Time taken to do 60000:70000: 0.5589091777801514
Time taken to do 70000:80000: 0.48449015617370605
Time taken to do 80000:90000: 0.543027400970459
Time taken to do 90000:100000: 0.5296523571014404
Time taken to do 100000:110000: 0.6949601173400879
Time taken to do 110000:120000: 1.437162160873413
Time taken to do 120000:130000: 1.2379283905029297
Time taken to do 130000:140000: 1.15696120262146
Time taken to make all the queries: 9.848754644393921
</code></pre>
<p>The final CSV looked like this:</p>
<pre><code>linkID;count(linkID);avg(absVelocity);geom
17341260;3;25.53091932029027;LINESTRING (-4.17935 53.21241, -4.17968 53.21257, -4.18033 53.21294, -4.18115 53.21319, -4.18138 53.2133, -4.18184 53.21339, -4.18204 53.21346, -4.18262 53.21369, -4.18334 53.21389, -4.18394 53.21439, -4.18473 53.21465, -4.18535 53.21475, -4.18618 53.21478)
17341303;1;31.053999640650744;LINESTRING (-4.17386 53.2051, -4.17501 53.20569, -4.17523 53.2058, -4.17534 53.20584)
</code></pre>
<p>The LINESTRING is a <a href="https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry">Well-Known Text (WKT) geometry
representation</a>
and corresponds to the coordinates that a road occupies.</p>
<h2>Future extensions</h2>
<p>Now that the groundwork is done, future extensions become merely an issue of
writing new code in Spark (which is, IMO, the easiest part). Some very nice and
(relatively) low-hanging fruit to pluck include:</p>
<ul>
<li>Subdividing average speeds by time of day (so we can see how traffic
conditions vary with time). We can even do a nice animation.</li>
<li>Bringing in the full database of millions of anonymised trips in the corpus (possibly by connecting
directly to the AWS servers rather than using a local MinIO instance)</li>
</ul>
<p>Additionally, we can use the calculated average speeds to give a more
personalised metric of driver safety. At the moment, a driver's score is
determined (in part) by whether or not he drives over the speed limit of a
road. But many roads do not fit their posted speed limits. It would be more
enlightening to compare a driver's driving patterns to a reference distribution
of other drivers on the same roads---and this data analysis helps us create
that reference distribution.</p>
<p>Finally, we could use the ground truth of average speeds as input to a machine
learning model. Can we predict the probability of a driver getting into an
accident given the roads that he drives on, the speed he drives on those roads,
and how he drives on them? A reference distribution is surely instructive here:
it's not as dangerous to go fast when everyone else around you is going fast,
but it's very dangerous to go fast when everyone else is slow (or go slow when
everyone else is fast).</p>
<p>From Richard:</p>
<blockquote>
<p>I think big data analysis is a definite. GPUs will be faster than CPU at
certain simple tasks so worth keeping to tasks that distribute well across CPUs
(small input data, lots of complex math, small output data / results). Inference
technology is also getting cheaper and dedicated neural network hardware
could be added to each node. Like the Google fpga or Intel hardware. this may be
faster than GPU</p>
</blockquote>
<blockquote>
<p>you should mention that Spark was bare metal and that the next step will be
to migrate to Docker Swarm deployed Spark nodes</p>
</blockquote>
<h2>What I learned</h2>
<p>Scala, Spark and distributed computing were completely new to me, and I had a
great time learning them. Learning about the abstractions of functional
programming expanded my mind. For instance, I had heard about monoids and
monads before but I only now understand their significance. Something that
really clicked for me was an explanation of how monoids map easily to
parallel programming, due to their associativity and identity.</p>
<p>Spark was cool as well. There were many helpful tips in the course about
optimising one's Spark program (always try to use Pair RDDs/Datasets, avoid
shuffles whenever possible, minimise the data sent over the network, using
range partitioning...) but sadly none of them were useful for this project.</p>
<p>It was difficult trying to get Spark working on the Raspberry Pi cluster---in
part because not many people have done it---but I'm pleased to have cracked
this tough nut. In fact the project made me want to build my own Raspi
cluster...</p>
<h2>Conclusion</h2>
<p>I am very happy to have had the opportunity to work on this project. I am very
grateful to Richard Jelbert for giving me this very cool project to work on.</p>
<p>Richard seems to be glad too. Running data analysis on the cluster with Spark
has been on his wishlist for quite a while now, but nobody has had the
bandwidth to do it.</p>

        <div>
      </div>

      <div class = "footer">
      </div>

    <div>


    </div>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!--Dark mode toggle -->
    <script src="/css/dark_mode_toggle.js"></script>

  </body>
</html>
