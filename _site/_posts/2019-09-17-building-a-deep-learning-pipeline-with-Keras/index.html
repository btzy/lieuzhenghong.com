<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="/css/reset.css" type="text/css">
    <link rel="stylesheet" href="/css/vars.css" type="text/css">
    <link rel="stylesheet" href="/css/base.css" type="text/css">

    <!-- sidebar -->
    <link rel="stylesheet" href="/css/nav.css" type="text/css">

    <!-- syntax highlighting -->
    <link href="https://unpkg.com/prismjs@1.20.0/themes/prism-okaidia.css" rel="stylesheet">

    <!-- font -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;1,100;1,300&display=swap" rel="stylesheet"> 

    <!-- Dark mode : script 1 of 2-->
    <script src="/css/dark_mode_init.js"></script>

    <title>Building a deep learning pipeline + model with Keras and PlaidML</title>
  </head>
  <body>

    <div class = "container">
      <div class = "header">
      </div>

      <div class = "body">
      <div class = "sidebar">
      <link rel="stylesheet" href="/css/nav.css" type="text/css">

<!-- Switch for dark mode toggle -->
<link rel="stylesheet" href="/css/switch.css" type="text/css">

<div>
 <a class = "page-title" href = "/"> 劉 </a>
 <span class = "page-subtitle"> Lieu Zheng Hong </span>
</div>


<hr>


<ul class = "navlist">
    <li> <a href="/"> Home </a> </li>
    <li> <a href="/about"> About </a> </li>
    <li> <a href="/projects"> Projects </a> </li>
    <li> <a href="/archive"> Archive </a> </li>
    <li> <a href="/ppe-stuff"> PPE work </li>
    <li> <a href="/notes"> Notes </a> </li>
</ul>

<hr>


<div class = "dark-mode-toggle-area">

  dark mode:
  <label id="theme-switch" class="theme-switch" for="checkbox_theme">
  <input type="checkbox" id="checkbox_theme" class="switch">
  <span class="slider"></span>
  </label>

</div>

      </div>

      <div class = "content-wrapper">
        <div class = "content">
          <h1> Building a deep learning pipeline + model with Keras and PlaidML </h1>
          <p>I built and iterated on a deep learning model that replicates the outputs of
an existing rule-based system while having 100x faster inference. As a
prerequisite to iterating on the model I also built a deep learning pipeline
that did:</p>
<ul>
<li>Data reading and cleaning</li>
<li>Feature extraction and selection</li>
<li>Data normalisation</li>
<li>Model checkpointing and saving</li>
</ul>
<p>I iterated over 6 major model configurations (training ~50 models overall) and
got the model RMSE to fall from 0.23 (simple RNN) to 0.128. This significantly
improves upon a baseline model (RMSE = 0.22) but there is still room for improvement.</p>
<p>The pipeline I built will speed up future iteration significantly. I will
continue the project with the Oxford Strategy Group (Digital), where I might
lead the team of consultants.</p>
<p>(Summer 2019 internship with Inzura)</p>
<hr>
<h2>Motivation</h2>
<p>Inzura AI has a proprietary algorithm called the &quot;Driver Profiler&quot;, which is a
complex rule-based system that measures how drivers drive. It derives
quantities like acceleration and velocity from GPS data. It looks up driver
position in a road network database to check for speeding (this road network
database allowed me to do data analysis on average speeds in <a href="/2019/09/12/big-data-analysis-on-a-distributed-raspi-cluster.html">another project I
did for
Inzura</a>).</p>
<p>From all this data it identifies speeding/braking/accelerating/turning events
and scores them. For instance a hard acceleration would be scored poorly.</p>
<p>Finally, it aggregates all the events to give a &quot;trip score&quot;, which is how
safely the driver drove on the trip.</p>
<p>This rule-based system works well but has two limitations. First, the database
lookups take a lot of time (latency) and necessitate an always-online device.</p>
<p>Second, most of the code for the driver profiler is written in Python. Richard
Jelbert, the CEO and co-founder of Inzura, wanted to deploy a version of the
driver profiler on dashcams. However, dashcams are embedded systems that run
C/C++.</p>
<p>A deep learning model solves both of these problems in one fell swoop. Once
trained, it can run inference completely offline, and there are libraries that
compile Keras models to C++. Additionally, offline inference means <em>real-time
inference</em>: the model could provide real-time feedback to a driver on his
performance, which is something that was impossible before.</p>
<h2>Feature selection / extraction</h2>
<p>There was a lot of data in every second of the trip, but I restricted myself to
values that could be calculated relatively quickly and did not require any
database lookups. That meant only things like velocity, acceleration, turn
radius, not road type or speed limit.</p>
<p>I eventually settled on four features:</p>
<ol>
<li>Absolute velocity</li>
<li>Tangential acceleration</li>
<li>Radial acceleration</li>
<li>Angle</li>
</ol>
<h2>Data cleaning</h2>
<p>There were anomalies in the data: whitespace issues, null values, wrong values,
etc. I had to write code that fixed all of that, which was trivial but tedious. [^1]</p>
<p>[^1]: I watched <a href="https://www.youtube.com/watch?v=MiiWzJE0fEA">this Youtube talk</a> on a probabilistic programming language that can <em>infer</em> missing data using Bayesian techniques. This is definitely something I want to explore.</p>
<h2>Keras generator</h2>
<p>I wrote a Python generator to read the training data into my Keras model.</p>
<p>A
<a href="https://stackoverflow.com/questions/1756096/understanding-generators-in-python">generator</a>
is a &quot;function which returns an object on which you can call next, such that
for every call it returns some value, until it raises a <code>StopIteration</code>
exception, signaling that all values have been generated. Such an object is
called an iterator.&quot;</p>
<p>The big benefit of a generator is that they deal with data one piece at a time,
and so we don't have to put the whole dataset into memory. With a small number
of trips this isn't a problem---50,000 trips may only take up ~2GB in
memory---but will be an issue in the future.</p>
<p>One hiccup: A generator must be infinite in order to work with Keras. That is,
when it reaches the end of the training data is must loop back again to the
start. This took some effort to code (I discovered this <a href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly#">very helpful
link</a>
too late) but will be very helpful when we start training with millions of
trips.</p>
<p>[TODO] code here</p>
<h2>Normalisation</h2>
<p>[TODO] code here</p>
<p>I wrote some simple code to calculate the mean and variance of each feature. I
fed the mean and variances into the Keras generator in order to normalise each
feature before it was used to train the model.</p>
<p>This was actually quite a slow process: it took about 30 minutes to run. (This
is to be expected as there are ~340 million numbers to add). This is a prime
candidate for parallelisation with the <a href="2019/09/12/big-data-analysis-on-a-distributed-raspi-cluster.html">Raspberry Pi 4 cluster
computer</a>.</p>
<h2>Model checkpointing</h2>
<p><a href="https://machinelearningmastery.com/check-point-deep-learning-models-keras/">Checkpointing my
model</a>
helped to save my progress by constantly saving intermediate models. What you
do is you train on the training set and only save a model when the validation
loss is lower than the best validation loss so far. That means that one can
save the best-performing model without worrying about overfitting.</p>
<p>But of course this meant that my initial train/test split would not work. I had
to divide it into train/validation/test with a 40000/5000/5000 split.</p>
<p>Here's what checkpointing looks like:</p>
<pre><code>Epoch 00033: val_loss improved from 0.14147 to 0.14128, saving model to cnn_6.7-33-0.1413.hdf5
Epoch 34/50
200/200 [==============================] - 465s 2s/step - loss: 0.1418 - val_loss: 0.1359

Epoch 00034: val_loss improved from 0.14128 to 0.13595, saving model to cnn_6.7-34-0.1359.hdf5
Epoch 35/50
200/200 [==============================] - 440s 2s/step - loss: 0.1391 - val_loss: 0.1395
</code></pre>
<h2>Iteration</h2>
<p>I started off with 10,000 trips. Each trip has length anywhere between
180-10000+ seconds. I extracted the four features at every time step. Null
values were taken to be 0.</p>
<p>The ground truth is <em>six</em> subscores that range from 0 to 1: my models try to
minimise the root-mean-squared-error (RMSE) between its predicted subscores and
the actual subscores.</p>
<h3>Mk I: Simple RNN</h3>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Returns a 6x1 vector: predicting 6 different scores</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>loss<span class="token operator">=</span>root_mean_squared_error<span class="token punctuation">)</span></code></pre>
<p>Train and test loss 0.23. I had to abandon the RNN because the training was too
slow. It took 40 minutes to train one epoch of 10,000 trips. This is because
each trip had a different length and so I couldn't increase the batch size. We
could have padded with 0s, but at the time I didn't want to, and decided to
move to CNNs instead.</p>
<p>I thought a CNN architecture might work here because I knew the rule-based
system was detecting &quot;events&quot;.</p>
<h3>Mk II: CNN with linear activation</h3>
<p>Train and test loss 0.22---not much difference from the RNN model, but by
introducing padding I was able to increase the batch size and thus reduce
training time by 10x, to 3-4 minutes per epoch.</p>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span><br>				 input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>loss<span class="token operator">=</span>root_mean_squared_error<span class="token punctuation">)</span></code></pre>
<h3>Mk III: CNN with RELU</h3>
<p>Changing the convolutional layers to use RELU rather than linear activation
functions made a big difference (thanks Jon Chuang), reducing the loss from
0.23 to 0.20 --- just scraping a win against the naive prediction. (Mk III).
Furthermore, it seems like I would get more gains from training with more
epochs.</p>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><br>				 padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">,</span><br>				 input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><br>				 padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><br>				 padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><br>				 padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv1D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><br>				 padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>loss<span class="token operator">=</span>root_mean_squared_error<span class="token punctuation">)</span></code></pre>
<h3>Mk IV: CNN with many more filters and dropout</h3>
<p>(Henceforth, I'll be omitting the code because all CNNs look the same, I was
just tweaking the parameter size and adding more layers)</p>
<p>I then moved in a different direction --- instead of going deeper (adding more
layers), I went wide (adding more filters). This was Mk. IV with 4 CNN layers
and 100/160 filters. Sadly, the model didn’t perform that well. But I learned
two things from this:</p>
<ol>
<li>Training speed was not affected! In other words, adding more filters gives
you more power “for free” --- at the cost of memory.</li>
<li>Wide filters do not overfit --- in other words, go hog wild with adding
larger layers.</li>
</ol>
<h3>Mk V: CNN with batch normalisation</h3>
<p>Emboldened by this, I decided to build Mk. V: exactly like Mk III, but
increasing the size of the convolutional layers.</p>
<p>I trained 7 more models, trying a combination of different techniques:</p>
<ul>
<li>5.1 add dropout layer --- training loss 0.2044, test loss 0.2050</li>
<li>5.2 change max pool layers to have size =2</li>
<li>5.3 change stride size = 1: training and test loss 0.20</li>
<li>5.4 append another Conv1D layer at the end with stride size = 2, training
loss 0.2042, test loss also 0.204</li>
<li>5.5 remove pooling layers : almost bricks the computer, huge-ass dense layer
(2.6 million parameters): training loss 0.1964, test loss 0.2146</li>
<li>5.6 add back the pooling layers: training loss 0.1977, test loss 0.200</li>
<li>5.7 add batch norm in between Conv1D layers.</li>
</ul>
<p>I knew that all of these models were underfitting as they were not doing much
better than the benchmark. So I decided to use MORE layers.</p>
<h3>Mk VI: CNN with MORE layers</h3>
<pre class="language-python"><code class="language-python">_________________________________________________________________<br>Layer <span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">)</span>                 Output Shape              Param <span class="token comment">#</span><br><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><br>conv1d_1 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">2100</span><br>_________________________________________________________________<br>conv1d_2 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">50100</span><br>_________________________________________________________________<br>max_pooling1d_1 <span class="token punctuation">(</span>MaxPooling1 <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">2500</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">0</span><br>_________________________________________________________________<br>conv1d_3 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">2500</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">50100</span><br>_________________________________________________________________<br>conv1d_4 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">2500</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">50100</span><br>_________________________________________________________________<br>max_pooling1d_2 <span class="token punctuation">(</span>MaxPooling1 <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1250</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">0</span><br>_________________________________________________________________<br>conv1d_5 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1250</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">50100</span><br>_________________________________________________________________<br>conv1d_6 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1250</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>         <span class="token number">50100</span><br>_________________________________________________________________<br>max_pooling1d_3 <span class="token punctuation">(</span>MaxPooling1 <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">625</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">0</span><br>_________________________________________________________________<br>conv1d_7 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">625</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">50100</span><br>_________________________________________________________________<br>conv1d_8 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">625</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">50100</span><br>_________________________________________________________________<br>max_pooling1d_4 <span class="token punctuation">(</span>MaxPooling1 <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">312</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">0</span><br>_________________________________________________________________<br>conv1d_9 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>            <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">312</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">50100</span><br>_________________________________________________________________<br>conv1d_10 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>           <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">312</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">50100</span><br>_________________________________________________________________<br>max_pooling1d_5 <span class="token punctuation">(</span>MaxPooling1 <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">156</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>          <span class="token number">0</span><br>_________________________________________________________________<br>conv1d_11 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>           <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">156</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span>           <span class="token number">7515</span><br>_________________________________________________________________<br>conv1d_12 <span class="token punctuation">(</span>Conv1D<span class="token punctuation">)</span>           <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span>            <span class="token number">1140</span><br>_________________________________________________________________<br>dropout_1 <span class="token punctuation">(</span>Dropout<span class="token punctuation">)</span>          <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span>            <span class="token number">0</span><br>_________________________________________________________________<br>flatten_1 <span class="token punctuation">(</span>Flatten<span class="token punctuation">)</span>          <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1170</span><span class="token punctuation">)</span>              <span class="token number">0</span><br>_________________________________________________________________<br>dense_1 <span class="token punctuation">(</span>Dense<span class="token punctuation">)</span>              <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>                <span class="token number">74944</span><br>_________________________________________________________________<br>dense_2 <span class="token punctuation">(</span>Dense<span class="token punctuation">)</span>              <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>                 <span class="token number">390</span><br><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span></code></pre>
<ul>
<li>Mk 6.1: I removed batch normalisation and added more layers for a total of 12
convolution layers. This gave me the lowest test loss so far. <strong>Train/test loss 0.1928/0.1857.</strong></li>
</ul>
<p>The reason why the test loss is smaller than the training loss is because of
dropout. From <a href="https://stats.stackexchange.com/a/205831/229360">a StackOverflow post</a>:</p>
<blockquote>
<p>One possibility: If you are using dropout regularization layer in
your network, it is reasonable that the validation error is smaller than
training error. Because usually dropout is activated when training but
deactivated when evaluating on the validation set. You get a more smooth
(usually means better) function in the latter case.</p>
</blockquote>
<ul>
<li>
<p>Mk 6.2: I trained for more epochs (40 rather than 20) and got a train/test
loss of <strong>0.1698/0.1820</strong>. I realised that the model was beginning to overfit
and so I got 40,000 more trips.</p>
</li>
<li>
<p>Mk 6.3: I trained for 50 epochs on 50,000 trips. Training time per epoch took
~20 minutes, which meant the entire training took <strong>15 hours</strong>. I got a
training loss of <strong>0.163</strong> and a test loss of <strong>0.1599</strong>. This is good news
--- adding more data stopped the model from overfitting.</p>
</li>
<li>
<p>Mk 6.4: forgot to write what I did</p>
</li>
<li>
<p>Mk 6.5: Instead of batch normalisation, I normalised <em>all</em> the inputs and
trained the same model again. This really helped! The neural network was able
to achieve the lowest-ever training and test loss. <strong>Training loss: 0.1475.
Test loss: 0.1280</strong></p>
</li>
<li>
<p>Mk 6.6: removed dropout layer and trained for 25 epochs.</p>
</li>
<li>
<p>Mk 6.7: Instead of stopping the training at a specific number of epochs, I
decided to start checkpointing, and continuously train until the validation
loss plateaued. In this way I could plot a <a href="https://stats.stackexchange.com/questions/51490/how-large-a-training-set-is-needed/51527#51527">learning
curve</a>
and determine whether I needed more data or a more powerful model.</p>
<p>Here I made <a href="#pointing-the-validation-set-to-be-the-same-as-the-test-set">silly embarassing mistake #2</a>. I fixed it but didn't have the time to rerun the training.</p>
</li>
</ul>
<p>I stopped the deep learning project here to focus on the Raspberry Pi cluster project.</p>
<h2>Silly, embarassing mistakes I made</h2>
<h3>Not taking advantage of multiprocessing</h3>
<p>In the <code>predict_generator</code>
<a href="https://keras.io/models/model/#predict_generator">function</a> there are
several arguments you can pass it to enable multi-CPU operation. One argument
was <code>multiprocessing=True</code> which I did set. But what I had neglected to
change was <code>workers</code>, which defaults to 1 if unspecified. Once I set
<code>workers=10</code> we got---predictably---a 10x speedup in training.</p>
<h3>Using the same data for both the validation and test set</h3>
<p>A silly mistake I made with list slicing meant that my validation set was the
same as my test set. I saw the validation loss keep decreasing to 0.09 and
was incredibly pleased; I thought it was too good to be true. It was!</p>
<h2>My thoughts</h2>
<p>Before I started this project I had a naive idea of deep learning. I was
implicitly assuming the following:</p>
<ol>
<li>Nice clean-ish data</li>
<li>Nice environment to do deep learning already set up for you</li>
<li>Nice well-defined requirements and unambiguous loss metric</li>
</ol>
<p>None of those assumptions held true in this project. In particular I had to
set up CUDA on my machine (to enable GPU acceleration) and that was really
difficult. Eventually I used PlaidML instead of Tensorflow for the backend
because it played nice with my GPU.</p>
<p>There's <em>so much glue code</em> that goes into a &quot;business&quot; machine learning
model. The actual Keras machine learning code that specifies the model is only
a couple dozen lines, but I have hundreds of lines of code that clean the data,
normalise the data, choose the right subset of the data, read the data, save
the model, run predictions with that model, ... and so on.</p>
<p>Finally, there are many insidious bugs that can bite you silently in deep
learning projects. This is probably due to my relative inexperience---I hope
that by documenting the silly mistakes here, I'll not make them again.</p>
<h2>Things I've learned</h2>
<p>This was the first non-toy deep learning project I have done. Because there
was no senior data scientist to guide me, I made many silly mistakes and
didn't follow best practices.</p>
<p>The most novel part for me was learning about, and writing, the architecture
that supports the deep learning model.</p>
<p>Input normalisation improves training significantly. I saw the training/test
loss fall from 0.16 to 0.13 just by normalising the input.</p>
<p>In the course of the project I went through some of the courses in Andrew
Ng's <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning
Specialisation</a>. I
didn't find the specialisation very useful but it was good in giving me some
deep learning intuitions.</p>
<h2>Conclusion</h2>
<p>Although I didn't get to finish the project, I still learned a lot. I had to
touch every single part of the deep learning pipeline---not just the model
code itself---which I really appreciated. A big thanks to Richard for
giving me this opportunity.</p>

        <div>
      </div>

      <div class = "footer">
      </div>

    <div>


    </div>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!--Dark mode toggle -->
    <script src="/css/dark_mode_toggle.js"></script>

  </body>
</html>
